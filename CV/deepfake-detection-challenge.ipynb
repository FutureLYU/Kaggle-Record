{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/efficientnettfkeras/efficientnet-master\r\n",
      "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.1.0) (1.0.8)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.1.0) (0.16.2)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.17.4)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (2.9.0)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (5.4.1)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (3.0.3)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (1.3.3)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (2.6.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (2.4)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.1.0) (1.1.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.13.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.4.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.8.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.0) (4.4.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (42.0.1.post20191125)\r\n",
      "Building wheels for collected packages: efficientnet\r\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.0-cp36-none-any.whl size=18328 sha256=e383409f9c1780b793b3146ddffeaf39d0f342ee98c257b1294f474a34cde62b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/99/7a/0d26ac8df8ca7b42784e5bf2dc85c0b35e37567de8eef7310c\r\n",
      "Successfully built efficientnet\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/efficientnettfkeras/efficientnet-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Face Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile('../input/mobilenet-face/frozen_inference_graph_face.pb', 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x7fa59709c5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = detection_graph.as_default()\n",
    "cm.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "num_detections = detection_graph.get_tensor_by_name('num_detections:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "ef1c1fd7-b763-4a40-a5c0-524bfbc503e0",
    "_uuid": "ffb09dc9-4297-4768-97e3-a9a9a8f37fe0"
   },
   "outputs": [],
   "source": [
    "def get_img(images):\n",
    "    global boxes,scores,num_detections\n",
    "    im_heights,im_widths=[],[]\n",
    "    imgs=[]\n",
    "    for image in images:\n",
    "        (im_height,im_width)=image.shape[:-1]\n",
    "        imgs.append(image)\n",
    "        im_heights.append(im_height)\n",
    "        im_widths.append(im_widths)\n",
    "    imgs=np.array(imgs)\n",
    "    (boxes, scores_) = sess.run(\n",
    "        [boxes_tensor, scores_tensor],\n",
    "        feed_dict={image_tensor: imgs})\n",
    "    finals=[]\n",
    "    for x in range(boxes.shape[0]):\n",
    "        scores=scores_[x]\n",
    "        max_=np.where(scores==scores.max())[0][0]\n",
    "        box=boxes[x][max_]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                      ymin * im_height, ymax * im_height)\n",
    "        left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n",
    "        image=imgs[x]\n",
    "        finals.append(cv2.cvtColor(cv2.resize(\n",
    "            image[max([0,top-40]):bottom+80,max([0,left-40]):right+80],(240,240)),cv2.COLOR_BGR2RGB))\n",
    "    return finals\n",
    "def detect_video(video):\n",
    "    frame_count=10\n",
    "    capture = cv2.VideoCapture(video)\n",
    "    v_len = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0,v_len,frame_count, endpoint=False, dtype=np.int)\n",
    "    imgs=[]\n",
    "    i=0\n",
    "    for frame_idx in range(int(v_len)):\n",
    "        ret = capture.grab()\n",
    "        if not ret: \n",
    "            print(\"Error grabbing frame %d from movie %s\" % (frame_idx, video))\n",
    "        if frame_idx >= frame_idxs[i]:\n",
    "            if frame_idx-frame_idxs[i]>20:\n",
    "                return None\n",
    "            ret, frame = capture.retrieve()\n",
    "            if not ret or frame is None:\n",
    "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, video))\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                imgs.append(frame)\n",
    "            i += 1\n",
    "            if i >= len(frame_idxs):\n",
    "                break\n",
    "    imgs=get_img(imgs)\n",
    "    if len(imgs)<10:\n",
    "        return None\n",
    "    return np.hstack(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daca41319aba4655a582e56258b2a529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.mkdir('./videos/')\n",
    "for x in tqdm(glob.glob('../input/deepfake-detection-challenge/test_videos/*.mp4')):\n",
    "    try:\n",
    "        filename=x.replace('../input/deepfake-detection-challenge/test_videos/','').replace('.mp4','.jpg')\n",
    "        a=detect_video(x)\n",
    "        if a is None:\n",
    "            continue\n",
    "        cv2.imwrite('./videos/'+filename,a)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.__exit__(None,Exception,'exit')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "class SeqSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    ATTENTION_TYPE_ADD = 'additive'\n",
    "    ATTENTION_TYPE_MUL = 'multiplicative'\n",
    "\n",
    "    def __init__(self,\n",
    "                 units=32,\n",
    "                 attention_width=None,\n",
    "                 attention_type=ATTENTION_TYPE_ADD,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 use_additive_bias=True,\n",
    "                 use_attention_bias=True,\n",
    "                 attention_activation=None,\n",
    "                 attention_regularizer_weight=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Layer initialization.\n",
    "        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n",
    "        :param units: The dimension of the vectors that used to calculate the attention weights.\n",
    "        :param attention_width: The width of local attention.\n",
    "        :param attention_type: 'additive' or 'multiplicative'.\n",
    "        :param return_attention: Whether to return the attention weights for visualization.\n",
    "        :param history_only: Only use historical pieces of data.\n",
    "        :param kernel_initializer: The initializer for weight matrices.\n",
    "        :param bias_initializer: The initializer for biases.\n",
    "        :param kernel_regularizer: The regularization for weight matrices.\n",
    "        :param bias_regularizer: The regularization for biases.\n",
    "        :param kernel_constraint: The constraint for weight matrices.\n",
    "        :param bias_constraint: The constraint for biases.\n",
    "        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n",
    "                                  in additive mode.\n",
    "        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n",
    "        :param attention_activation: The activation used for calculating the weights of attention.\n",
    "        :param attention_regularizer_weight: The weights of attention regularizer.\n",
    "        :param kwargs: Parameters for parent class.\n",
    "        \"\"\"\n",
    "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "        self.attention_width = attention_width\n",
    "        self.attention_type = attention_type\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        if history_only and attention_width is None:\n",
    "            self.attention_width = int(1e9)\n",
    "\n",
    "        self.use_additive_bias = use_additive_bias\n",
    "        self.use_attention_bias = use_attention_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
    "        self.attention_activation = keras.activations.get(attention_activation)\n",
    "        self.attention_regularizer_weight = attention_regularizer_weight\n",
    "        self._backend = keras.backend.backend()\n",
    "\n",
    "        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self.Wx, self.Wt, self.bh = None, None, None\n",
    "            self.Wa, self.ba = None, None\n",
    "        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self.Wa, self.ba = None, None\n",
    "        else:\n",
    "            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'attention_width': self.attention_width,\n",
    "            'attention_type': self.attention_type,\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "            'use_additive_bias': self.use_additive_bias,\n",
    "            'use_attention_bias': self.use_attention_bias,\n",
    "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
    "            'attention_activation': keras.activations.serialize(self.attention_activation),\n",
    "            'attention_regularizer_weight': self.attention_regularizer_weight,\n",
    "        }\n",
    "        base_config = super(SeqSelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self._build_additive_attention(input_shape)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self._build_multiplicative_attention(input_shape)\n",
    "        super(SeqSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def _build_additive_attention(self, input_shape):\n",
    "        feature_dim = int(input_shape[2])\n",
    "\n",
    "        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wt'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wx'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_additive_bias:\n",
    "            self.bh = self.add_weight(shape=(self.units,),\n",
    "                                      name='{}_Add_bh'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(self.units, 1),\n",
    "                                  name='{}_Add_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Add_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def _build_multiplicative_attention(self, input_shape):\n",
    "        feature_dim = int(input_shape[2])\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n",
    "                                  name='{}_Mul_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Mul_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        input_len = K.shape(inputs)[1]\n",
    "\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            e = self._call_additive_emission(inputs)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            e = self._call_multiplicative_emission(inputs)\n",
    "\n",
    "        if self.attention_activation is not None:\n",
    "            e = self.attention_activation(e)\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        if self.attention_width is not None:\n",
    "            if self.history_only:\n",
    "                lower = K.arange(0, input_len) - (self.attention_width - 1)\n",
    "            else:\n",
    "                lower = K.arange(0, input_len) - self.attention_width // 2\n",
    "            lower = K.expand_dims(lower, axis=-1)\n",
    "            upper = lower + self.attention_width\n",
    "            indices = K.expand_dims(K.arange(0, input_len), axis=0)\n",
    "            e = e * K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx())\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n",
    "\n",
    "        # a_{t} = \\text{softmax}(e_t)\n",
    "        s = K.sum(e, axis=-1, keepdims=True)\n",
    "        a = e / (s + K.epsilon())\n",
    "\n",
    "        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n",
    "        v = K.batch_dot(a, inputs)\n",
    "        if self.attention_regularizer_weight > 0.0:\n",
    "            self.add_loss(self._attention_regularizer(a))\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [v, a]\n",
    "        return v\n",
    "\n",
    "    def _call_additive_emission(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size, input_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n",
    "        q = K.expand_dims(K.dot(inputs, self.Wt), 2)\n",
    "        k = K.expand_dims(K.dot(inputs, self.Wx), 1)\n",
    "        if self.use_additive_bias:\n",
    "            h = K.tanh(q + k + self.bh)\n",
    "        else:\n",
    "            h = K.tanh(q + k)\n",
    "\n",
    "        # e_{t, t'} = W_a h_{t, t'} + b_a\n",
    "        if self.use_attention_bias:\n",
    "            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n",
    "        else:\n",
    "            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n",
    "        return e\n",
    "\n",
    "    def _call_multiplicative_emission(self, inputs):\n",
    "        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n",
    "        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n",
    "        if self.use_attention_bias:\n",
    "            e += self.ba[0]\n",
    "        return e\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        if self.return_attention:\n",
    "            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def _attention_regularizer(self, attention):\n",
    "        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n",
    "        input_len = K.shape(attention)[-1]\n",
    "        indices = K.expand_dims(K.arange(0, input_len), axis=0)\n",
    "        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)\n",
    "        eye = K.cast(K.equal(indices, diagonal), K.floatx())\n",
    "        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n",
    "            attention,\n",
    "            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_custom_objects():\n",
    "        return {'SeqSelfAttention': SeqSelfAttention}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_B1model():\n",
    "    bottleneck = efn.EfficientNetB1(weights=None,include_top=False,pooling='avg')\n",
    "    inp=Input((10,240,240,3))\n",
    "    x=TimeDistributed(bottleneck)(inp)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "    x = SeqSelfAttention(attention_activation='sigmoid')(x)\n",
    "    x = Dense(64, activation='elu')(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    model=Model(inp,x)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_B2model():\n",
    "    bottleneck = efn.EfficientNetB2(weights=None,include_top=False,pooling='avg')\n",
    "    inp=Input((10,240,240,3))\n",
    "    x=TimeDistributed(bottleneck)(inp)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "    x = SeqSelfAttention(attention_activation='sigmoid')(x)\n",
    "    x = Dense(64, activation='elu')(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    model=Model(inp,x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = create_B2model()\n",
    "model1.load_weights('../input/tcnnaws/TCNN-AWS-B2f1.08-0.1526.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = create_B2model()\n",
    "model2.load_weights('../input/tcnnaws/TCNN-AWS-B2f2-cc.01-0.1566.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_2 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = create_B2model()\n",
    "model3.load_weights('../input/tcnnaws/TCNN-AWS-B2f3.09-0.1560.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_3 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = create_B2model()\n",
    "model4.load_weights('../input/tcnnaws/TCNN-AWS-B2f4.12-0.1484.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_4 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = create_B2model()\n",
    "model5.load_weights('../input/tcnnaws/TCNN-AWS-B2f5.06-0.1671.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_5 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6 = create_B2model()\n",
    "model6.load_weights('../input/tcnnaws/efficientnetB2f1-biLSTM-aug.10-0.1812.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_6 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model7 = create_B2model()\n",
    "model7.load_weights('../input/tcnnaws/TCNN-AWS-B2f2-aug-c.07-0.1667.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_7 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model8 = create_B2model()\n",
    "model8.load_weights('../input/tcnnaws/efficientnetB2f3-biLSTM-aug.09-0.1858.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_8 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model9 = create_B2model()\n",
    "model9.load_weights('../input/tcnnaws/efficientnetB2f4-biLSTM-aug.01-0.1834.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 10, 1408)          7768562   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_9 (SeqSel (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 9,375,412\n",
      "Trainable params: 9,307,844\n",
      "Non-trainable params: 67,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model10 = create_B2model()\n",
    "model10.load_weights('../input/tcnnaws/TCNN-AWS-B2f5-aug.13-0.1662.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 10, 1280)          6575232   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 10, 256)           1442816   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_10 (SeqSe (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 8,051,010\n",
      "Trainable params: 7,988,962\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model11 = create_B1model()\n",
    "model11.load_weights('../input/tcnnaws/TCNN-AWS4.09-0.1872.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 10, 1280)          6575232   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 10, 256)           1442816   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_11 (SeqSe (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 8,051,010\n",
      "Trainable params: 7,988,962\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model12 = create_B1model()\n",
    "model12.load_weights('../input/tcnnaws/TCNN-AWS-f2.07-0.1875.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 10, 1280)          6575232   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 10, 256)           1442816   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_12 (SeqSe (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 8,051,010\n",
      "Trainable params: 7,988,962\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model13 = create_B1model()\n",
    "model13.load_weights('../input/tcnnaws/TCNN-AWS-f3.12-0.1820.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 10, 240, 240, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 10, 1280)          6575232   \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 10, 256)           1442816   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_13 (SeqSe (None, None, 256)         16449     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 8,051,010\n",
      "Trainable params: 7,988,962\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model14 = create_B1model()\n",
    "model14.load_weights('../input/tcnnaws/TCNN-AWS-f3.09-0.1742.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12, model13, model14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_birghtness(img):\n",
    "    return img/img.max()\n",
    "\n",
    "# %% [code]\n",
    "def process_img(img,flip=False):\n",
    "    imgs=[]\n",
    "    for x in range(10):\n",
    "        if flip:\n",
    "            imgs.append(get_birghtness(cv2.flip(img[:,x*240:(x+1)*240,:],1)))\n",
    "        else:\n",
    "            imgs.append(get_birghtness(img[:,x*240:(x+1)*240,:]))\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/deepfake-detection-challenge/sample_submission.csv\")\n",
    "test_files=glob.glob('./videos/*.jpg')\n",
    "submission=pd.DataFrame()\n",
    "submission['filename']=os.listdir(('../input/deepfake-detection-challenge/test_videos/'))\n",
    "submission['label']=0.5\n",
    "filenames=[]\n",
    "batch=[]\n",
    "batch1=[]\n",
    "preds=[]\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d650896b2e49e08230cf3a5ec8bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear-10\n",
      "clear-20\n",
      "clear-30\n",
      "clear-40\n",
      "clear-50\n",
      "clear-60\n",
      "clear-70\n",
      "clear-80\n",
      "clear-90\n",
      "clear-100\n",
      "clear-110\n",
      "clear-120\n",
      "clear-130\n",
      "clear-140\n",
      "clear-150\n",
      "clear-160\n",
      "clear-170\n",
      "clear-180\n",
      "clear-190\n",
      "clear-200\n",
      "clear-210\n",
      "clear-220\n",
      "clear-230\n",
      "clear-240\n",
      "clear-250\n",
      "clear-260\n",
      "clear-270\n",
      "clear-280\n",
      "clear-290\n",
      "clear-300\n",
      "clear-310\n",
      "clear-320\n",
      "clear-330\n",
      "clear-340\n",
      "clear-350\n",
      "clear-360\n",
      "clear-370\n",
      "clear-380\n",
      "clear-390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(test_files):\n",
    " \n",
    "    img=process_img(cv2.cvtColor(cv2.imread(x),cv2.COLOR_BGR2RGB))\n",
    "    if img is None:\n",
    "        continue\n",
    "    batch.append(img)\n",
    "    batch1.append(process_img(cv2.cvtColor(cv2.imread(x),cv2.COLOR_BGR2RGB),True))\n",
    "    filenames.append(x.replace('./videos/','').replace('.jpg','.mp4'))\n",
    "    if len(batch)==16:\n",
    "        result = 0\n",
    "        for model in models:\n",
    "            result += (model.predict(np.array(batch))+model.predict(np.array(batch1)))\n",
    "        preds+=(result/(2*len(models))).tolist()\n",
    "        #preds+=((model.predict(np.array(batch)))).tolist()\n",
    "        batch=[]\n",
    "        batch1=[]\n",
    "        del result\n",
    "    if i % 10 == 0 and i // 10 > 0:\n",
    "        gc.collect()\n",
    "        print(f'clear-{i}')\n",
    "    i += 1\n",
    "if len(batch)!=0:\n",
    "    result = 0\n",
    "    for model in models:\n",
    "        result += (model.predict(np.array(batch))+model.predict(np.array(batch1)))\n",
    "    preds+=(result/(2*len(models))).tolist()\n",
    "    #preds+=((model.predict(np.array(batch)))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4820"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519886860253755\n"
     ]
    }
   ],
   "source": [
    "new_preds=[]\n",
    "for x in preds:\n",
    "    new_preds.append(x[0][0])\n",
    "print(sum(new_preds)/len(new_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(new_preds,filenames):\n",
    "    submission.loc[submission['filename']==y,'label']=min([max([0.01,x]),0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([106.,  42.,  18.,  11.,   6.,   3.,   5.,   5.,   2.,   1.,   4.,\n",
       "          5.,   0.,   0.,   1.,   4.,   3.,   7.,  16., 161.]),\n",
       " array([0.01 , 0.059, 0.108, 0.157, 0.206, 0.255, 0.304, 0.353, 0.402,\n",
       "        0.451, 0.5  , 0.549, 0.598, 0.647, 0.696, 0.745, 0.794, 0.843,\n",
       "        0.892, 0.941, 0.99 ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(submission['label'],20,histtype = 'bar', facecolor = 'yellowgreen',alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nymodlmxni.mp4</td>\n",
       "      <td>0.052544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oaguiggjyv.mp4</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bdshuoldwx.mp4</td>\n",
       "      <td>0.966336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fwykevubzy.mp4</td>\n",
       "      <td>0.179550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mllzkpgatp.mp4</td>\n",
       "      <td>0.980953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>pqthmvwonf.mp4</td>\n",
       "      <td>0.981431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>xugmhbetrw.mp4</td>\n",
       "      <td>0.029129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>orekjthsef.mp4</td>\n",
       "      <td>0.965992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>yhjlnisfel.mp4</td>\n",
       "      <td>0.029527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>nswtvttxre.mp4</td>\n",
       "      <td>0.103021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename     label\n",
       "0    nymodlmxni.mp4  0.052544\n",
       "1    oaguiggjyv.mp4  0.010000\n",
       "2    bdshuoldwx.mp4  0.966336\n",
       "3    fwykevubzy.mp4  0.179550\n",
       "4    mllzkpgatp.mp4  0.980953\n",
       "..              ...       ...\n",
       "395  pqthmvwonf.mp4  0.981431\n",
       "396  xugmhbetrw.mp4  0.029129\n",
       "397  orekjthsef.mp4  0.965992\n",
       "398  yhjlnisfel.mp4  0.029527\n",
       "399  nswtvttxre.mp4  0.103021\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185351888186298"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(submission['label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading. Please upvote if you found it helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1038ff1967fc403d8adf3a1bb4d26a80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2134b3580c6c48df96778465a37839ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "311dda02e0b64751b5195198afdda347": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "361d7840e95842afa45f2efcdfdd8019": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2134b3580c6c48df96778465a37839ad",
       "placeholder": "",
       "style": "IPY_MODEL_6e70c4d66136452d934b67454eab7bc9",
       "value": " 400/400 [14:49&lt;00:00,  2.22s/it]"
      }
     },
     "406525f3028249298e99f7412718dd09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4132add745174b76ab6bbce32c94532e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ed94288a2a474abfa792195eaa820f8b",
       "max": 400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_43dce98a49784ed39c686d3cc2b7827a",
       "value": 400
      }
     },
     "43dce98a49784ed39c686d3cc2b7827a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5b6ae66832a5426ba067c8140471d3b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_406525f3028249298e99f7412718dd09",
       "max": 400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d5b9683137ca4fb0ab7b5f1fce1c68a9",
       "value": 400
      }
     },
     "6e70c4d66136452d934b67454eab7bc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c1d650896b2e49e08230cf3a5ec8bdc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5b6ae66832a5426ba067c8140471d3b8",
        "IPY_MODEL_c295a877152446a7b1116ab4691a715b"
       ],
       "layout": "IPY_MODEL_1038ff1967fc403d8adf3a1bb4d26a80"
      }
     },
     "c295a877152446a7b1116ab4691a715b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e06964ed3a744a02937a97b653a03ac9",
       "placeholder": "",
       "style": "IPY_MODEL_c87a71ea814e4dddb137dadffb74bcfe",
       "value": " 400/400 [09:59&lt;00:00,  1.50s/it]"
      }
     },
     "c87a71ea814e4dddb137dadffb74bcfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d5b9683137ca4fb0ab7b5f1fce1c68a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "daca41319aba4655a582e56258b2a529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4132add745174b76ab6bbce32c94532e",
        "IPY_MODEL_361d7840e95842afa45f2efcdfdd8019"
       ],
       "layout": "IPY_MODEL_311dda02e0b64751b5195198afdda347"
      }
     },
     "e06964ed3a744a02937a97b653a03ac9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed94288a2a474abfa792195eaa820f8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
